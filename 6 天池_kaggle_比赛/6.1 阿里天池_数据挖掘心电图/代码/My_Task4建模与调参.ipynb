{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 学习目标\n",
    "- 学习机器学习模型的建模过程与调参流程\n",
    "- 完成相应学习打卡任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 内容介绍\n",
    "- 逻辑回归模型：\n",
    "\n",
    "  - 理解逻辑回归模型；\n",
    "  - 逻辑回归模型的应用；\n",
    "  - 逻辑回归的优缺点；\n",
    "\n",
    "- 树模型：\n",
    "\n",
    "  - 理解树模型；\n",
    "  - 树模型的应用；\n",
    "  - 树模型的优缺点；\n",
    "\n",
    "- 集成模型\n",
    "\n",
    "  - 基于bagging思想的集成模型\n",
    "    - 随机森林模型\n",
    "  - 基于boosting思想的集成模型\n",
    "    - XGBoost模型\n",
    "    - LightGBM模型\n",
    "    - CatBoost模型\n",
    "\n",
    "- 模型对比与性能评估：\n",
    "\n",
    "  - 回归模型/树模型/集成模型；\n",
    "  - 模型评估方法；\n",
    "  - 模型评价结果；\n",
    "\n",
    "- 模型调参：\n",
    "\n",
    "  - 贪心调参方法；\n",
    "\n",
    "  - 网格调参方法；\n",
    "\n",
    "  - 贝叶斯调参方法； "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 代码示例\n",
    "## 4.5.1 导入相关和相关设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 读取数据\n",
    "reduce_mean_usage 函数通过调整数据类型,帮助我们减少数据在内存中占用的空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('./train.csv')\n",
    "# 简单预处理\n",
    "data_list = []\n",
    "for items in data.values:\n",
    "    data_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_'+str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 简单建模\n",
    "基于树模型的算法特性，异常值、缺失值处理可以跳过，但是对于业务较为了解的同学也可以自己对缺失异常值进行处理，效果可能会更优于模型处理的结果。\n",
    "\n",
    "注：以下建模的据集并未构造任何特征，直接使用原特征。本次主要任务还是模建模调参。\n",
    "- 建模之前的预操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# 分离数据集,方便进行交叉验证\n",
    "x_train = data.drop(['id','label'],axis=1)\n",
    "y_train = data['label']\n",
    "\n",
    "# 五折交叉验证\n",
    "folds = 5 \n",
    "seed = 2021\n",
    "kf = KFold(n_splits = folds,shuffle=True,random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>s_0</th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>s_3</th>\n",
       "      <th>s_4</th>\n",
       "      <th>s_5</th>\n",
       "      <th>s_6</th>\n",
       "      <th>s_7</th>\n",
       "      <th>s_8</th>\n",
       "      <th>...</th>\n",
       "      <th>s_196</th>\n",
       "      <th>s_197</th>\n",
       "      <th>s_198</th>\n",
       "      <th>s_199</th>\n",
       "      <th>s_200</th>\n",
       "      <th>s_201</th>\n",
       "      <th>s_202</th>\n",
       "      <th>s_203</th>\n",
       "      <th>s_204</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991211</td>\n",
       "      <td>0.943359</td>\n",
       "      <td>0.764648</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.379639</td>\n",
       "      <td>0.190796</td>\n",
       "      <td>0.040222</td>\n",
       "      <td>0.026001</td>\n",
       "      <td>0.031708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971680</td>\n",
       "      <td>0.929199</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>0.178467</td>\n",
       "      <td>0.122986</td>\n",
       "      <td>0.132324</td>\n",
       "      <td>0.094421</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.030487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958984</td>\n",
       "      <td>0.701172</td>\n",
       "      <td>0.231812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080688</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.280762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.975586</td>\n",
       "      <td>0.934082</td>\n",
       "      <td>0.659668</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>0.237061</td>\n",
       "      <td>0.281494</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>0.249878</td>\n",
       "      <td>0.241455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055817</td>\n",
       "      <td>0.261230</td>\n",
       "      <td>0.359863</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.453613</td>\n",
       "      <td>0.499023</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.616699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.677734</td>\n",
       "      <td>0.222412</td>\n",
       "      <td>0.257080</td>\n",
       "      <td>0.204712</td>\n",
       "      <td>0.054657</td>\n",
       "      <td>0.026154</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.244873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996.0</td>\n",
       "      <td>0.926758</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.637207</td>\n",
       "      <td>0.415039</td>\n",
       "      <td>0.374756</td>\n",
       "      <td>0.382568</td>\n",
       "      <td>0.358887</td>\n",
       "      <td>0.341309</td>\n",
       "      <td>0.336426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997.0</td>\n",
       "      <td>0.925781</td>\n",
       "      <td>0.587402</td>\n",
       "      <td>0.633301</td>\n",
       "      <td>0.632324</td>\n",
       "      <td>0.639160</td>\n",
       "      <td>0.614258</td>\n",
       "      <td>0.599121</td>\n",
       "      <td>0.517578</td>\n",
       "      <td>0.403809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994629</td>\n",
       "      <td>0.829590</td>\n",
       "      <td>0.458252</td>\n",
       "      <td>0.264160</td>\n",
       "      <td>0.240234</td>\n",
       "      <td>0.213745</td>\n",
       "      <td>0.189331</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999.0</td>\n",
       "      <td>0.925781</td>\n",
       "      <td>0.916504</td>\n",
       "      <td>0.404297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.385498</td>\n",
       "      <td>0.361084</td>\n",
       "      <td>0.332764</td>\n",
       "      <td>0.339844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       s_0       s_1       s_2       s_3       s_4       s_5  \\\n",
       "0          0.0  0.991211  0.943359  0.764648  0.618652  0.379639  0.190796   \n",
       "1          1.0  0.971680  0.929199  0.572754  0.178467  0.122986  0.132324   \n",
       "2          2.0  1.000000  0.958984  0.701172  0.231812  0.000000  0.080688   \n",
       "3          3.0  0.975586  0.934082  0.659668  0.249878  0.237061  0.281494   \n",
       "4          4.0  0.000000  0.055817  0.261230  0.359863  0.433105  0.453613   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "99995  99995.0  1.000000  0.677734  0.222412  0.257080  0.204712  0.054657   \n",
       "99996  99996.0  0.926758  0.906250  0.637207  0.415039  0.374756  0.382568   \n",
       "99997  99997.0  0.925781  0.587402  0.633301  0.632324  0.639160  0.614258   \n",
       "99998  99998.0  1.000000  0.994629  0.829590  0.458252  0.264160  0.240234   \n",
       "99999  99999.0  0.925781  0.916504  0.404297  0.000000  0.262939  0.385498   \n",
       "\n",
       "            s_6       s_7       s_8  ...  s_196  s_197  s_198  s_199  s_200  \\\n",
       "0      0.040222  0.026001  0.031708  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "1      0.094421  0.089600  0.030487  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "2      0.128418  0.187500  0.280762  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "3      0.249878  0.249878  0.241455  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "4      0.499023  0.542969  0.616699  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "...         ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "99995  0.026154  0.118164  0.244873  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "99996  0.358887  0.341309  0.336426  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "99997  0.599121  0.517578  0.403809  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "99998  0.213745  0.189331  0.203857  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "99999  0.361084  0.332764  0.339844  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       s_201  s_202  s_203  s_204  label  \n",
       "0        0.0    0.0    0.0    0.0    0.0  \n",
       "1        0.0    0.0    0.0    0.0    0.0  \n",
       "2        0.0    0.0    0.0    0.0    2.0  \n",
       "3        0.0    0.0    0.0    0.0    0.0  \n",
       "4        0.0    0.0    0.0    0.0    2.0  \n",
       "...      ...    ...    ...    ...    ...  \n",
       "99995    0.0    0.0    0.0    0.0    0.0  \n",
       "99996    0.0    0.0    0.0    0.0    2.0  \n",
       "99997    0.0    0.0    0.0    0.0    3.0  \n",
       "99998    0.0    0.0    0.0    0.0    2.0  \n",
       "99999    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[100000 rows x 207 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 因为树模型中没有f1-score评价指标，所以需要自定义评价指标，在模型迭代中返回验证集f1-score变化情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_vail(preds,data_vali):\n",
    "    labels = data_vali.get_label()  # 这里reshape()将其转换为4行一列\n",
    "    preds = np.argmax(preds.reshape(4,-1),axis=0)\n",
    "    score_vail = f1_score(y_true=labels,y_pred=preds,average = 'macro')\n",
    "    return 'f1_score',score_vail,True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用Lightgbm进行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.048446\tvalid_0's f1_score: 0.96088\n",
      "[100]\tvalid_0's multi_logloss: 0.0430823\tvalid_0's f1_score: 0.966723\n",
      "[150]\tvalid_0's multi_logloss: 0.0448085\tvalid_0's f1_score: 0.969935\n",
      "[200]\tvalid_0's multi_logloss: 0.0464403\tvalid_0's f1_score: 0.971709\n",
      "[250]\tvalid_0's multi_logloss: 0.0478184\tvalid_0's f1_score: 0.972054\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's multi_logloss: 0.0428137\tvalid_0's f1_score: 0.965724\n"
     ]
    }
   ],
   "source": [
    "\"\"\"对训练集数据进行划分，分成训练集和验证集，并进行相应的操作\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# 数据集划分  validation 验证\n",
    "x_train_split,x_val,y_train_split,y_val = train_test_split(x_train,y_train,test_size=0.2)\n",
    "train_matrix = lgb.Dataset(x_train_split,label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(x_val,label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"boosting\": 'gbdt',  \n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 128,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"metric\": None,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 4,\n",
    "    \"nthread\": 10,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "model = lgb.train(params,\n",
    "                  train_set=train_matrix,\n",
    "                  valid_sets=valid_matrix,\n",
    "                  num_boost_round=2000,\n",
    "                  verbose_eval=50,\n",
    "                  early_stopping_rounds=200,\n",
    "                  feval=f1_score_vail\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对验证集进行预测\n",
    "\n",
    "a = np.array([[1, 5, 5, 2],\n",
    "              [9, 6, 2, 8],\n",
    "              [3, 7, 9, 1]])  \n",
    "b=np.argmax(a, axis=0)#对二维矩阵来讲a[0][1]会有两个索引方向，第一个方向为a[0]，默认按列方向搜索最大值   \n",
    "#a的第一列为1，9，3,最大值为9，所在位置为1，   \n",
    "#a的第一列为5，6，7,最大值为7，所在位置为2，   \n",
    "#此此类推，因为a有4列，所以得到的b为1行4列，   \n",
    "print(b)#[1 2 2 1]   \n",
    " \n",
    "c=np.argmax(a, axis=1)#现在按照a[0][1]中的a[1]方向，即行方向搜索最大值，   \n",
    "#a的第一行为1，5，5，2,最大值为5（虽然有2个5，但取第一个5所在的位置），索引值为1， \n",
    "#a的第2行为9，6，2，8,最大值为9，索引值为0，  \n",
    "#因为a有3行，所以得到的c有3个值，即为1行3列  \n",
    "\n",
    "argmax：https://blog.csdn.net/weixin_38145317/article/details/79650188  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未调参前lightgbm单模型在验证集上的f1：0.9657243693065285\n"
     ]
    }
   ],
   "source": [
    "val_pre_lgb = model.predict(x_val, num_iteration=model.best_iteration)\n",
    "preds = np.argmax(val_pre_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print('未调参前lightgbm单模型在验证集上的f1：{}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 更进一步的,使用5折交叉验证进行建模预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0408155\tvalid_0's f1_score: 0.966797\n",
      "[200]\tvalid_0's multi_logloss: 0.0437957\tvalid_0's f1_score: 0.971239\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0406453\tvalid_0's f1_score: 0.967452\n",
      "[0.9674515729721614]\n",
      "************************************ 2 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0472933\tvalid_0's f1_score: 0.965828\n",
      "[200]\tvalid_0's multi_logloss: 0.0514952\tvalid_0's f1_score: 0.968138\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's multi_logloss: 0.0467472\tvalid_0's f1_score: 0.96567\n",
      "[0.9674515729721614, 0.9656700872844327]\n",
      "************************************ 3 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0378154\tvalid_0's f1_score: 0.971004\n",
      "[200]\tvalid_0's multi_logloss: 0.0405053\tvalid_0's f1_score: 0.973736\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's multi_logloss: 0.037734\tvalid_0's f1_score: 0.970004\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769]\n",
      "************************************ 4 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0495142\tvalid_0's f1_score: 0.967106\n",
      "[200]\tvalid_0's multi_logloss: 0.0542324\tvalid_0's f1_score: 0.969746\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's multi_logloss: 0.0490886\tvalid_0's f1_score: 0.965566\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014]\n",
      "************************************ 5 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0412544\tvalid_0's f1_score: 0.964054\n",
      "[200]\tvalid_0's multi_logloss: 0.0443025\tvalid_0's f1_score: 0.965507\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0411855\tvalid_0's f1_score: 0.963114\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_scotrainre_list:[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_score_mean:0.9663612141019279\n",
      "lgb_score_std:0.0022854824074775683\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "for i,(train_index,valid_index) in enumerate(kf.split(x_train,y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    x_train_split,y_train_split,x_val,y_val = x_train.iloc[train_index],y_train[train_index],x_train.iloc[valid_index],y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(x_train_split,label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(x_val,label=y_val)\n",
    "    \n",
    "    params = {\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"boosting\": 'gbdt',  \n",
    "                \"lambda_l2\": 0.1,\n",
    "                \"max_depth\": -1,\n",
    "                \"num_leaves\": 128,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"metric\": None,\n",
    "                \"objective\": \"multiclass\",\n",
    "                \"num_class\": 4,\n",
    "                \"nthread\": 10,\n",
    "                \"verbose\": -1,\n",
    "            }\n",
    "    \n",
    "    model = lgb.train(\n",
    "                       params,\n",
    "                       train_set=train_matrix,\n",
    "                       valid_sets=valid_matrix,\n",
    "                       num_boost_round=2000,\n",
    "                       verbose_eval=100,\n",
    "                       early_stopping_rounds=200,\n",
    "                       feval=f1_score_vail\n",
    "    )\n",
    "    val_pred = model.predict(x_val,num_iteration=model.best_iteration)\n",
    "    \n",
    "    val_pred = np.argmax(val_pred,axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.4 模型调参\n",
    "- 1.贪心调参\n",
    "先使用当前对模型影响最大的参数进行调参,达到当前参数下的模型最优化,再使用对模型影响次之的参数进行调参,如此下去,直到所有的参数调整完毕.   \n",
    "\n",
    "这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。\n",
    "\n",
    "需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，这里列举一下日常调参过程中常用的参数和调参顺序：\n",
    "\n",
    "- ①：max_depth,num_leaves\n",
    "- ②：min_data_in_leaf,min_child_weight\n",
    "- ③：bagging_fraction,feature_fraction,bagging_freq\n",
    "- ④：reg_lambda、reg_alpha\n",
    "- ⑤：min_split_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'objective' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dba9ecf3027e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 调objective\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbest_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"\"\"预测并计算roc的相关指标\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'objective' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# 调objective\n",
    "best_obj = dict()\n",
    "for obj in objective:\n",
    "    model = LGBMRegressor(objective=obj)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_obj[obj] = score\n",
    "\n",
    "# num_leaves\n",
    "best_leaves = dict()\n",
    "for leaves in num_leaves:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_leaves[leaves] = score\n",
    "\n",
    "# max_depth\n",
    "best_depth = dict()\n",
    "for depth in max_depth:\n",
    "    model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],\n",
    "                          num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],\n",
    "                          max_depth=depth)\n",
    "    \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "    best_depth[depth] = score\n",
    "\n",
    "\"\"\"\n",
    "可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可依次将模型的参数通过上面的方式进行调整优化,并且通过可视化观察在每一个最优参数下模型的得分情况   \n",
    "- 2.网格搜索  \n",
    "sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果。  \n",
    "  \n",
    "同样以Lightgbm算法为例，进行网格搜索调参：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"通过网格搜索确定最优参数\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def get_best_cv_params(learning_rate=0.1, n_estimators=581,num_leaves=31,max_depth=-1,bagging_fraction=1.0,feature_fraction=1.0,bagging_fraction_freq=0,min_data_in_leaf=20,min_child_weight=0.001,min_split_gain=0,reg_lambda=0,reg_alpha=0,pagram_grid=None):\n",
    "    # 设置5折交叉验证\n",
    "    cv_fold = KFold(n_splits=5,shuffle=True,random_state=2021)\n",
    "    \n",
    "    model_lgb = lgb.LGBMClassifier(learning_rate = learning_rate,\n",
    "                                   n_estimators = n_estimators,\n",
    "                                   num_leaves = num_leaves,\n",
    "                                   max_depth = max_depth,\n",
    "                                   bagging_fraction = bagging_fraction,\n",
    "                                   feature_fraction = feature_fraction,\n",
    "                                   bagging_freq = bagging_feq,\n",
    "                                   min_data_in_leaf = min_data_in_leaf,\n",
    "                                   min_child_weight = min_child_weight,\n",
    "                                   min_split_gain = min_split_gain,\n",
    "                                   reg_lambda = reg_lambda,\n",
    "                                   reg_alpha = reg_alpha,\n",
    "                                   n_jobs = 8)\n",
    "    \n",
    "    f1 = make_scorer(f1_score,average = 'micro')\n",
    "    grid_search = GridSearchCV(estimator = model_lgb,\n",
    "                               cv = cv_fold,\n",
    "                               pagram_grid = pagram_grid,\n",
    "                               scoring = f1)\n",
    "    \n",
    "    grid_search.fit(x_train,y_train)\n",
    "    \n",
    "    print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n",
    "    print('模型当前最优得分为:{}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"以下代码未运行，耗时较长，请谨慎运行，且每一步的最优参数需要在下一步进行手动更新，请注意\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "需要注意一下的是，除了获取上面的获取num_boost_round时候用的是原生的lightgbm（因为要用自带的cv）\n",
    "下面配合GridSearchCV时必须使用sklearn接口的lightgbm。\n",
    "\"\"\"\n",
    "\"\"\"设置n_estimators 为581，调整num_leaves和max_depth，这里选择先粗调再细调\"\"\"\n",
    "lgb_params = {'num_leaves': range(10, 80, 5), 'max_depth': range(3,10,2)}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                   min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"num_leaves为30，max_depth为7，进一步细调num_leaves和max_depth\"\"\"\n",
    "lgb_params = {'num_leaves': range(25, 35, 1), 'max_depth': range(5,9,1)}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                   min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定min_data_in_leaf为45，min_child_weight为0.001 ，下面进行bagging_fraction、feature_fraction和bagging_freq的调参\n",
    "\"\"\"\n",
    "lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "              'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "              'bagging_freq': range(0,81,10)\n",
    "             }\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=None, feature_fraction=None, bagging_freq=None, \n",
    "                   min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定bagging_fraction为0.4、feature_fraction为0.6、bagging_freq为 ，下面进行reg_lambda、reg_alpha的调参\n",
    "\"\"\"\n",
    "lgb_params = {'reg_lambda': [0,0.001,0.01,0.03,0.08,0.3,0.5], 'reg_alpha': [0,0.001,0.01,0.03,0.08,0.3,0.5]}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                   min_split_gain=0, reg_lambda=None, reg_alpha=None, param_grid=lgb_params)\n",
    "\n",
    "\"\"\"\n",
    "确定reg_lambda、reg_alpha都为0，下面进行min_split_gain的调参\n",
    "\"\"\"\n",
    "lgb_params = {'min_split_gain': [i/10 for i in range(0,11,1)]}\n",
    "get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                   min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                   min_split_gain=None, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(10, 80, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "参数确定好了以后，我们设置一个比较小的learning_rate 0.005，来确定最终的num_boost_round\n",
    "\"\"\"\n",
    "# 设置5折交叉验证\n",
    "# cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n",
    "final_params = {\n",
    "                 'boosting_type':'gbdt',\n",
    "                 'learning_rate':0.01,\n",
    "                 'num_leaves':29,\n",
    "                 'max_depth':7,\n",
    "                 'objective':'multiclass',\n",
    "                 'num_class':4,\n",
    "                 'min_data_in_leaf':45,\n",
    "                 'min_child_weight':0.001,\n",
    "                 'bagging_fraction':0.9,\n",
    "                 'feature_fraction':0.9,\n",
    "                 'bagging_freq':40,\n",
    "                 'min_split_gain':0,\n",
    "                 'reg_lambda':0,\n",
    "                 'reg_alpha':0,\n",
    "                 'nthread':6\n",
    "}\n",
    "\n",
    "cv_result = lgb.cv(train_set=lgb_train,\n",
    "                   early_stopping_rounds=20,\n",
    "                   num_boost_round=5000,\n",
    "                   nfold=5,\n",
    "                   stratified=True,\n",
    "                   params = final_params,\n",
    "                   feval=f1_score_vali,\n",
    "                   seed=0,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际调整过程中,可先设置一个较大的学习率(上面的例子中0.1),通过Lgb原生的cv函数进行树个数的确定,之后再通过上面的示例代码进行参数的调整优化\n",
    "\n",
    "最后针对最优的参数设置一个较小的学习率（例如0.05），同样通过cv函数确定树的个数，确定最终的参数。\n",
    "\n",
    "需要注意的是，针对大数据集，上面每一层参数的调整都需要耗费较长时间，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **贝叶斯调参**  \n",
    "在使用之前需要先安装包bayesian-optimization，运行如下命令即可："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯调参的主要思想是：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。\n",
    "\n",
    "贝叶斯调参的步骤如下： 😅\n",
    "\n",
    "- 定义优化函数(rf_cv）\n",
    "- 建立模型\n",
    "- 定义待优化的参数\n",
    "- 得到优化结果，并返回要优化的分数指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\"\"\"定义优化函数\"\"\"\n",
    "def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, \n",
    "              min_child_weight, min_split_gain, reg_lambda, reg_alpha):\n",
    "    # 建立模型\n",
    "    model_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_class=4,\n",
    "                                   learning_rate=0.1, n_estimators=5000,\n",
    "                                   num_leaves=int(num_leaves), max_depth=int(max_depth), \n",
    "                                   bagging_fraction=round(bagging_fraction, 2),                      \t                                    feature_fraction=round(feature_fraction, 2),\n",
    "                                   bagging_freq=int(bagging_freq),                                                                          min_data_in_leaf=int(min_data_in_leaf),\n",
    "                                   min_child_weight=min_child_weight, min_split_gain=min_split_gain,\n",
    "                                   reg_lambda=reg_lambda, reg_alpha=reg_alpha,\n",
    "                                   n_jobs= 8\n",
    "                                  )\n",
    "    f1 = make_scorer(f1_score, average='micro')\n",
    "    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5, scoring=f1).mean()\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | featur... | max_depth | min_ch... | min_da... | min_sp... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Ljc\\Aacon\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.6508155736019849, 49.54695263456983, 0.893495504752377, 19.43624882376842, 7.359802977985826, 34.759929140940606, 0.9692072315173706, 111.96538389153179, 2.1595497164382427, 4.059557646933362)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2c2786145eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;34m\"\"\"开始优化\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mbayes_lgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Ljc\\Aacon\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ljc\\Aacon\\Anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Ljc\\Aacon\\Anaconda3\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-45eb6e6667c6>\u001b[0m in \u001b[0;36mrf_cv_lgb\u001b[1;34m(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, min_child_weight, min_split_gain, reg_lambda, reg_alpha)\u001b[0m\n\u001b[0;32m     14\u001b[0m                                    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                                   )\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_lgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\"\"\"定义优化参数\"\"\"\n",
    "bayes_lgb = BayesianOptimization(\n",
    "    rf_cv_lgb, \n",
    "    {\n",
    "        'num_leaves':(10, 200),\n",
    "        'max_depth':(3, 20),\n",
    "        'bagging_fraction':(0.5, 1.0),\n",
    "        'feature_fraction':(0.5, 1.0),\n",
    "        'bagging_freq':(0, 100),\n",
    "        'min_data_in_leaf':(10,100),\n",
    "        'min_child_weight':(0, 10),\n",
    "        'min_split_gain':(0.0, 1.0),\n",
    "        'reg_alpha':(0.0, 10),\n",
    "        'reg_lambda':(0.0, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"开始优化\"\"\"\n",
    "bayes_lgb.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"显示优化结果\"\"\"\n",
    "bayes_lgb.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数优化完成后,我们可以根据优化后的参数建立新的模型.降低学习率并寻找最优模型迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score_vali' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3920783d8434>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_params_lgb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1_score_vali\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_score_vali' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"调整一个较小的学习率，并通过cv函数确定当前最优的迭代次数\"\"\"\n",
    "base_params_lgb = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'multiclass',\n",
    "                    'num_class': 4,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'num_leaves': 138,\n",
    "                    'max_depth': 11,\n",
    "                    'min_data_in_leaf': 43,\n",
    "                    'min_child_weight':6.5,\n",
    "                    'bagging_fraction': 0.64,\n",
    "                    'feature_fraction': 0.93,\n",
    "                    'bagging_freq': 49,\n",
    "                    'reg_lambda': 7,\n",
    "                    'reg_alpha': 0.21,\n",
    "                    'min_split_gain': 0.288,\n",
    "                    'nthread': 10,\n",
    "                    'verbose': -1,\n",
    "}\n",
    "\n",
    "cv_result_lgb = lgb.cv(\n",
    "    train_set=train_matrix,\n",
    "    early_stopping_rounds=1000, \n",
    "    num_boost_round=20000,\n",
    "    nfold=5,\n",
    "    stratified=True,\n",
    "    shuffle=True,\n",
    "    params=base_params_lgb,\n",
    "    feval=f1_score_vali,\n",
    "    seed=0\n",
    ")\n",
    "print('迭代次数{}'.format(len(cv_result_lgb['f1_score-mean'])))\n",
    "print('最终模型的f1为{}'.format(max(cv_result_lgb['f1_score-mean'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型参数已经确定,建立最终模型并对验证集进行验证**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    x_train_split, y_train_split, x_val, y_val = x_train_split.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "\n",
    "    train_matrix = lgb.Dataset(x_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(x_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'multiclass',\n",
    "                'num_class': 4,\n",
    "                'learning_rate': 0.01,\n",
    "                'num_leaves': 138,\n",
    "                'max_depth': 11,\n",
    "                'min_data_in_leaf': 43,\n",
    "                'min_child_weight':6.5,\n",
    "                'bagging_fraction': 0.64,\n",
    "                'feature_fraction': 0.93,\n",
    "                'bagging_freq': 49,\n",
    "                'reg_lambda': 7,\n",
    "                'reg_alpha': 0.21,\n",
    "                'min_split_gain': 0.288,\n",
    "                'nthread': 10,\n",
    "                'verbose': -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(params, train_set=train_matrix, num_boost_round=4833, valid_sets=valid_matrix, \n",
    "                      verbose_eval=1000, early_stopping_rounds=200, feval=f1_score_vali)\n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型调参小总结**\n",
    "\n",
    "- 集成模型内置的cv函数可以较快的进行单一参数的调节，一般可以用来优先确定树模型的迭代次数\n",
    "\n",
    "- 数据量较大的时候（例如本次项目的数据），网格搜索调参会特别特别慢，不建议尝试\n",
    "\n",
    "- 集成模型中原生库和sklearn下的库部分参数不一致，需要注意，具体可以参考xgb和lgb的官方API\n",
    "\n",
    "  > [xgb原生库API](https://xgboost.readthedocs.io/en/stable/parameter.html)，[sklearn库下xgbAPI](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn)\n",
    "  >\n",
    "  > [lgb原生库API](https://lightgbm.readthedocs.io/en/latest/Parameters.html)， [sklearn库下lgbAPI](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6 经验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，我们主要完成了建模与调参的工作，首先在建模的过程中通过划分数据集、交叉验证等方式对模型的性能进行评估验证   \n",
    "\n",
    "最后我们对模型进行调参，这部分介绍了贪心调参、网格搜索调参、贝叶斯调参共三种调参手段，重点使用贝叶斯调参对本次项目进行简单优化，大家在实际操作的过程中可以参考调参思路进行优化，不必拘泥于以上教程所写的具体实例。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<1> 终于深刻体会那句我们是炼丹的[调参]    \n",
    "<2> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已,那么感觉特征工程和模型调参都贼难,努力成为一个优秀的炼丹师   \n",
    "<3> 不知道为什么将最优的参数尝试弄在Task1的baseline,结果反向上分了害   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
