### **1、建模目标**：
* **不要让模型欠拟合**
* **不要让模型过拟合**

* 学完这节课，当面试数据挖掘的内容时，
* 你不要说不知道数据挖掘流程是怎样的？
* 你不要说不知道EDA，特征工程等名词是什么？
* 你不要说不知道常规的EDA，特征工程等具体方法有哪些？

### 2、数据挖掘流程
* **确定目标（将业务问题转化为建模问题）**  --- **数据探索** --- **数据预处理** --- **特征工程** --- **模型训练**   --- **模型评估** --- （**模型融合**） --- **模型预测** --- **结果分析** --- **（迭代更新）**

### 3、数据探索（EDA：Exploratory Data Analysis）
**目的**：

* a.观察哪些数据要做预处理。
* b.大致了解数据分布情况，并对有效特征作记录。
* c.检查类别样本不均衡问题。
* 预测用户流失与否，10000个用户，100个用户流失，9900未流失 1:99。一般来说，1：10

**3.1、分析特征变量的分布：查看哪些特征是连续型，哪些是离散型。**（查看哪些需预处理）

* 连续型的特征：若标准差较大，可考虑归一化，标准化，对数变换等。
* 离散型的特征：观察每个离散值的频率分布，对于频次较低的特征，可以考虑统一编码为“其他”类别。

**3.2、分析目标变量的分布：（检查样本不均衡问题）**


* 若为连续型特征分析值域范围是否过大，是否存在异常值。
* 若为离散型特征，贯彻样本分类是否平衡。

**3.3、查看数据缺失值情况：哪些特征有缺失值，缺失值占比多少？（查看哪些需预处理）**


* （数据预处理补充）

**3.4、查看数据异常值？：（查看哪些需预处理）**


* 用1.5四分位间距、如果数据符合正态分布表，则查看数据是否超过3个标准差外、或则用异常检测算法等。

**3.5、分析变量之间两两分布和相关度。（有效特征记录）**


* 观察哪些变量与目标变量的相关性高。
* 观察特征之间的共线性。
* 查看变量与时间的关系。

**3.6、【重要】探索各特征对目标变量的影响（有效特征记录）**


* （实战案例补充）

### 4、数据预处理（Data Preprocessing）

* **目的**：让数据符合模型训练的基本要求，并让模型更好的学习数据。

* **4.1、数据集成**
* 数据挖掘需要的数据往往分布在不同数据源中，数据集成就是将多个数据源合并的过程。
* 冗余特征去除：同名异义、异名同义


* **4.2、缺失值处理**
* 缺失值太多（占比超50%），建议舍弃。缺失值转换成others
* 连续型：按不同的分布类型对缺失值进行补全：偏正态分布，使用均值代替，可以保持数据的均值；偏长尾分布，使用中值代替，避免受 outlier 的影响。
* 离散型：按频数最大的填充。
* 将缺失值当模型目标变量Y，其他特征为X，使用模型去预测。
* 根据业务的理解，使用固定值填充。


* **4.3、异常值处理**
* 删除含有异常值的记录
* 将异常值视为缺失值，交给缺失值处理方法处理
* 用平均数修正。
* 在不懂业务的情况：第一种是聚类，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本。我们可以将其从训练集过滤掉。
* 第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。


### 5、特征工程（feature engineering）

* **目的**：让模型更好的学习数据。

* 特征工程分层两大部分：**特征构建**与**特征选择**


* 一句老话：数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已

* 举例：比如我们目的是预测用户是否留存。
* 根据数据探索我们发现，男性留存率30%，女性留存率29%。北京用户留存率23%，深圳用户留存率25%。
* 然后我们再下钻分析发现，“北京且男性”的用户留存率为45%。远高于它们单维度时候的留存率。此时我们就构建一个“地区+性别”的组合特征。
* 有了这个特征，模型才能学习到这个规律。


```python
import pandas as pd
```


```python
df = pd.DataFrame({'用户id':[1,2,3],'SEX':['男','女','女']})
```


```python
df
```



</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户id</th>
      <th>SEX</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>男</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>女</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>女</td>
    </tr>
  </tbody>
</table>
</div>




```python
手机型号，iphone-12,11,10......
```



</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>用户id</th>
      <th>SEX_女</th>
      <th>SEX_男</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# top10的手机型号是哪些，别的直接用others。11独立元素，one-hotencode
df['手机型号'].value_counts()
```


```python
import numpy as np 
```


```python
np.log(100)
```




    4.605170185988092



#### 特征构建

##### **5.1、类别特征处理**


* labelEncoder
* one-hotencoding
* 只出现一次的类别处理：在类别特征列里，有时有一些类别，在训练集和测试集中总共只出现一次，例如特别偏僻的郊区地址。此时，保留其原有的自然数编码意义不大，不如将所有频数为1的类别合并到同一个新的类别下。
* 平均数编码：高势集类别可用经验贝叶斯转换成数值feature，粗略理解这个特征出现的概率。
* 参考url:https://zhuanlan.zhihu.com/p/26308272
* 什么是高势集类别？
* 一个简单的例子就是邮编，有100个城市就会有好几百个邮编，有些房子坐落在同一个邮编下面。很显然随着邮编的数量增多。如果用简单的one-hot编码显然效果不太好，有可能造成维度爆炸。
* 高势集类别满足条件：

1. 会重复
2. 根据相同的值分组会分出超过一定数量（比如100）的组。


* 统计每个取值在样本中出现的频率，取 Top N 的取值进行 One-hot 编码，剩下的类别分到“其他“类目下，其中 N 需要根据模型效果进行调优。

##### **5.2、数值特征处理**


* 标准化：有种情况，若观察到异常值太大，标准化计数时用中位数。可免疫异常值影响。
* 区间缩放（归一化），不免疫异常值。
![1611563983(1).jpg](https://i.loli.net/2021/01/25/KnDfZe5HLTvsrha.png)
* 二值化：特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0
* 离散化：如年龄特征区间分类，价格特征区间分类等，分箱
* 对数转换：单调性
* 数据变换：将数据多项式化，在输入数据中增加非线性特征可以有效的提高模型的复杂度。简单且常用的方法就是使用多项式特征（polynomial features),可以得到特征的高阶交叉项。
![1611564004(1).jpg](https://i.loli.net/2021/01/25/YvPhbQjRJGf6oty.png)

##### **5.3、处理样本不平稳问题**


* 权重法：权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。

* 采样法：

  1. **欠采样与过采样**

  **缺点：**都改变了数据分布。增大模型过拟合的可能，也可采样smote算法，基本不改变数据分布，

  2. **SMOTEENN类来做SMOTE采样。**(我们可以用imbalance-learn这个Python库中)

  **缺点**：非常慢

##### **5.4、通用特征工程套路**


* 统计特征：count，max，sum，avg，std，var等等。
* 比值特征：例如目前有转化人数及总人数特征，构建转化率。
* 排序特征：对数值型特征进行排序，可降低异常值或量纲的影响。
* 时间间额特征：分钟、小时、天、近三天、近一周、近一个月 + 指标

##### **5.5、其他特征**：


* 时间特征：针对于时间数据来讲，提取年、月、日、星期等可能还是不够的，有另外一些points可以去思考，用户的兴趣跟发布时间的久远是否有关系？残差分位数特征、残差的波峰波谷等特征。

* 地理位置特征：想到地理位置，就会想到聚类，一个简单的方式将每个房子划分到同一块区域中去；除了聚类以外，算出几个中心点坐标，计算曼哈顿距离或者欧式距离可能都会有神奇的效果。（与经纬度结合也是种办法，参考geohash算法）。

* 文本特征：词向量、提取关键词、文本相似度等。

##### **5.6、寻找高级特征（交叉特征、特征组合）：**


* 寻找高级特征最常用的方法有：
* 若干项特征加和： 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
* 若干项特征之差： 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
* 若干项特征乘积： 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
* 若干项特征除商： 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售。


#### 特征选择

目的：
* 1、减少特征数量，降低模型复杂度，减少方差，避免过拟合。
* 2、去除一些不相关，不重要的特征，提升模型的性能。


* 特征选择分三种：**过滤法**、**包装法**、**嵌入法**

##### **过滤法**：


* 方差选择法：方差越大的特征，那么我们可以认为它是比较有用的。
* 相关系数：这个主要用于输出连续值的监督学习算法中。我们分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征。
* 卡方检验：卡方检验可以检验某个特征分布和输出值分布之间的相关性，选择卡方值较大的部分特征。
* 互信息法：即从信息熵的角度分析各个特征和输出值之间的关系评分。在决策树算法中我们讲到过互信息（信息增益）。互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。

##### **包装法**：


* 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

##### **嵌入法**：


* 基于L1正则项的特征选择法
* 基于树模型的特征选择法


```python
正则项：L1，L2
* L1正则项:更狠，能够使特征稀疏化。w1x1+w2x2+w3x3+....wnxn，让特征权重为0
* L2正则项：让无效特征的权重下降。但是它不会让权重归0
```


      File "<ipython-input-10-fad51789f286>", line 1
        正则项：L1，L2
                ^
    SyntaxError: invalid character in identifier




```python
过采样：10000样本，1000留存 1000：9000，1：9。1:5，复制留存样本，添加到原数据集中，直到比例为1:5。>10000，13000样本量。
欠采样：样本数是小于原样本。      
```


```python
x1,x2,x3,x4,x5。
1、用所有特征，训练模型，输出评估指标s1
2、去除某一个特征，x1，训练模型，输出评估指标s2
3、s1-s2，假设变化，说明这个特征有没有你，都对指标没有什么影响。这个特征就是无效特征。
```

### 6、模型选择


* （1）从机器学习问题角度分类：该问题是监督还是无监督，或是分类、回归、聚类、还是降维所使用的方法都不同。
![1611566475_1_.jpg](https://i.loli.net/2021/01/25/TlIRO9t4VkdoAy2.png)


* （2）从数据量大小去选择模型。

![1611566616(1).jpg](https://i.loli.net/2021/01/25/ErPnos8wFu1W4pS.png)



* （3）从数据类型角度选择模型：如文本数据、图像数据、或是常规二维数据所选模型都不一样。


* （4）最常用的方法：根据模型的实际评分情况选择模型，可用交叉验证方法取各模型的得分，然后选取得分较高的模型。

### 7、模型融合：
* （1）Voting：模型融合其实也没有想象的那么高大上，从最简单的Voting说起，这也可以说是一种模型融合。假设对于一个二分类问题，有3个基础模型，那么就采取投票制的方法，投票多者确定为最终的分类。


* （2）Averaging：对于回归问题，一个简单直接的思路是取平均。稍稍改进的方法是进行加权平均。权值可以用排序的方法确定，举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6


* （3）Bagging：
* Bagging就是采用有放回的方式进行抽样，用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合。大概分为这样两步：
* 1、重复K次
* 有放回地重复抽样建模
* 训练子模型
* 2、模型融合
* 分类问题：voting
* 回归问题：average


* （4）Boosting：Bagging算法可以并行处理，而Boosting的思想是一种迭代的方法，每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。


* （5）Stacking：见Stacking附件。
